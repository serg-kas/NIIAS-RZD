{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "evzpadrdn0v5s98kkyhcyi",
    "execution_id": "fff859c5-b71a-4422-9be6-d6b7bc6c1b74"
   },
   "source": [
    "### Создание и обучение модели для сегментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cellId": "4l86dcv5z58oqcpmxc4j6p"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np \n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model       \n",
    "from tensorflow.keras.layers import Input, Conv2DTranspose, concatenate, Activation, MaxPooling2D, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Reshape, GlobalAveragePooling2D, UpSampling2D, AveragePooling2D\n",
    "from tensorflow.keras import backend as K        \n",
    "from tensorflow.keras.optimizers import Adam     \n",
    "from tensorflow.keras import utils               \n",
    "from tensorflow.keras.preprocessing import image \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "# from PIL import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cellId": "wnfb0hzdetjymu8mj9qz"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "SEED = 1111\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cellId": "bmwxv5kcvxspz51dvywuha"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Путь к сохраненным выборкам\n",
    "data_path = \"/home/jupyter/work/resources/NIIAS-RZD/data_np\"\n",
    "# Путь для сохранения моделей\n",
    "model_path = \"/home/jupyter/work/resources/NIIAS-RZD/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellId": "oo0pes17ezsjvld4jr06k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имеем 30 сохраненных выборок\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Список выборок\n",
    "np_files  = []\n",
    "for file_name in sorted(os.listdir(data_path)):\n",
    "    if 'data' in file_name:\n",
    "        np_files.append(file_name)\n",
    "print(\"Имеем {} сохраненных выборок\".format(len(np_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cellId": "bbaqagb30dit69m2i919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data07.npz\n",
      "(800, 432, 768, 3) (800, 432, 768, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.8/site-packages/ml_kernel/kernel.py:873: UserWarning: The following variables cannot be serialized: data\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Загрузим пример сохраненной выборки\n",
    "np_file = random.choice(np_files)\n",
    "print(np_file)\n",
    "data = np.load(os.path.join(data_path, np_file))\n",
    "images_np = data['a']\n",
    "anns_np = data['b']\n",
    "print(images_np.shape, anns_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cellId": "eqqugvy63qcdsihanih5ql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432 768\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Размеры, изображения\n",
    "img_height = images_np.shape[1]      # 432\n",
    "img_width = images_np.shape[2]       # 768\n",
    "print(img_height, img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cellId": "5jlwrrwadtwdoj34xw913s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Классов сегментации\n",
    "classes = [0, 6, 7, 10]\n",
    "num_classes = len(classes)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "5yj8ekt72sd1tvujjvzb1x",
    "execution_id": "bc90a1f0-e142-4a2a-9aea-0ba6c6f59267"
   },
   "source": [
    "### Создание Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cellId": "5658qykr0ye0vm771671l3g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Функция метрики, обрабатывающая пересечение двух областей\n",
    "def dice_coef(y_true, y_pred):\n",
    "  # Возвращаем площадь пересечения деленную на площадь объединения двух областей\n",
    "  return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cellId": "1o1k30yzdayn3gpfwe43u9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "'''\n",
    "  Функция создания сети\n",
    "    Входные параметры:\n",
    "    - num_classes - количество классов\n",
    "    - input_shape - размерность карты сегментации\n",
    "'''\n",
    "def unet(num_classes = 4, input_shape= (432, 768, 3)):\n",
    "    img_input = Input(input_shape)                                         # Создаем входной слой с размерностью input_shape\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='block1_conv1')(img_input) # Добавляем Conv2D-слой с 64-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='block1_conv2')(x)         # Добавляем Conv2D-слой с 64-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    block_1_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_1_out\n",
    "\n",
    "    x = MaxPooling2D()(block_1_out)                                        # Добавляем слой MaxPooling2D\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), padding='same', name='block2_conv1')(x)        # Добавляем Conv2D-слой с 128-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same', name='block2_conv2')(x)        # Добавляем Conv2D-слой с 128-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    block_2_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_2_out\n",
    "\n",
    "    x = MaxPooling2D()(block_2_out)                                        # Добавляем слой MaxPooling2D\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv1')(x)        # Добавляем Conv2D-слой с 256-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv2')(x)        # Добавляем Conv2D-слой с 256-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv3')(x)        # Добавляем Conv2D-слой с 256-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    block_3_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_3_out\n",
    "\n",
    "    x = MaxPooling2D()(block_3_out)                                        # Добавляем слой MaxPooling2D\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv1')(x)        # Добавляем Conv2D-слой с 512-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv2')(x)        # Добавляем Conv2D-слой с 256-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv3')(x)        # Добавляем Conv2D-слой с 256-нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    block_4_out = Activation('relu')(x)                                    # Добавляем слой Activation и запоминаем в переменной block_4_out\n",
    "    x = block_4_out \n",
    "\n",
    "    # UP 2\n",
    "    x = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(x)    # Добавляем слой Conv2DTranspose с 256 нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = concatenate([x, block_3_out])                                      # Объединем текущий слой со слоем block_3_out\n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)                             # Добавляем слой Conv2D с 256 нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    # UP 3\n",
    "    x = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)    # Добавляем слой Conv2DTranspose с 128 нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = concatenate([x, block_2_out])                                      # Объединем текущий слой со слоем block_2_out\n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)                             # Добавляем слой Conv2D с 128 нейронами\n",
    "    x = BatchNormalization()(x)                                            # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x)                                              # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(128, (3, 3), padding='same')(x) # Добавляем слой Conv2D с 128 нейронами\n",
    "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x) # Добавляем слой Activation\n",
    "\n",
    "    # UP 4\n",
    "    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(x) # Добавляем слой Conv2DTranspose с 64 нейронами\n",
    "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x) # Добавляем слой Activation\n",
    "\n",
    "    x = concatenate([x, block_1_out])  # Объединем текущий слой со слоем block_1_out\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x) # Добавляем слой Conv2D с 64 нейронами\n",
    "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x) # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x) # Добавляем слой Conv2D с 64 нейронами\n",
    "    x = BatchNormalization()(x) # Добавляем слой BatchNormalization\n",
    "    x = Activation('relu')(x) # Добавляем слой Activation\n",
    "\n",
    "    x = Conv2D(num_classes, (3, 3), activation='softmax', padding='same')(x)  # Добавляем Conv2D-Слой с softmax-активацией на num_classes-нейронов\n",
    "\n",
    "    model = Model(img_input, x) # Создаем модель с входом 'img_input' и выходом 'x'\n",
    "\n",
    "    # Компилируем модель \n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[dice_coef])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellId": "qby3r9gem1mp0bswi8gw5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 22:03:28.966935: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jupyter/work/resources/NIIAS-RZD/models/00-Unet/assets\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Создаем модель и сохраняем ее \n",
    "modelUnet = unet(num_classes, (img_height, img_width, 3))\n",
    "# modelPSPnet.summary()\n",
    "\n",
    "model_file = os.path.join(model_path, '00-Unet')\n",
    "modelUnet.save(model_file, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "cellId": "tujsit3rkkg11njyjboo9v8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 60s 178ms/step - loss: 0.2659 - dice_coef: 0.8615 - val_loss: 0.3127 - val_dice_coef: 0.8422\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.2538 - dice_coef: 0.8675 - val_loss: 0.2836 - val_dice_coef: 0.8552\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 60s 178ms/step - loss: 0.2512 - dice_coef: 0.8684 - val_loss: 0.2926 - val_dice_coef: 0.8680\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.2440 - dice_coef: 0.8734 - val_loss: 0.2685 - val_dice_coef: 0.8714\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.2335 - dice_coef: 0.8787 - val_loss: 0.3274 - val_dice_coef: 0.8489\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.2235 - dice_coef: 0.8821 - val_loss: 0.2891 - val_dice_coef: 0.8567\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.2184 - dice_coef: 0.8861 - val_loss: 0.2856 - val_dice_coef: 0.8763\n",
      "INFO:tensorflow:Assets written to: /home/jupyter/work/resources/NIIAS-RZD/models/04-Unet/assets\n",
      "\n",
      "Обучение на выборке data05.npz\n",
      "Epoch 1/10\n",
      "340/340 [==============================] - 62s 181ms/step - loss: 0.2422 - dice_coef: 0.8759 - val_loss: 0.3339 - val_dice_coef: 0.8382\n",
      "Epoch 2/10\n",
      "340/340 [==============================] - 60s 177ms/step - loss: 0.2314 - dice_coef: 0.8819 - val_loss: 0.3684 - val_dice_coef: 0.8568\n",
      "Epoch 3/10\n",
      "340/340 [==============================] - 60s 177ms/step - loss: 0.2259 - dice_coef: 0.8831 - val_loss: 0.3359 - val_dice_coef: 0.8547\n",
      "Epoch 4/10\n",
      "340/340 [==============================] - 60s 178ms/step - loss: 0.2196 - dice_coef: 0.8866 - val_loss: 0.3288 - val_dice_coef: 0.8674\n",
      "Epoch 5/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.2067 - dice_coef: 0.8924 - val_loss: 0.3091 - val_dice_coef: 0.8621\n",
      "Epoch 6/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.1988 - dice_coef: 0.8972 - val_loss: 0.3343 - val_dice_coef: 0.8754\n",
      "Epoch 7/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.1942 - dice_coef: 0.8989 - val_loss: 0.3309 - val_dice_coef: 0.8469\n",
      "Epoch 8/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.1896 - dice_coef: 0.9013 - val_loss: 0.3163 - val_dice_coef: 0.8592\n",
      "Epoch 9/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.1860 - dice_coef: 0.9027 - val_loss: 0.3673 - val_dice_coef: 0.8235\n",
      "Epoch 10/10\n",
      "340/340 [==============================] - 61s 178ms/step - loss: 0.1772 - dice_coef: 0.9068 - val_loss: 0.3280 - val_dice_coef: 0.8706\n",
      "INFO:tensorflow:Assets written to: /home/jupyter/work/resources/NIIAS-RZD/models/05-Unet/assets\n",
      "Время выполнения: 3288.79c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 22:04:52.580841: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-07-20 22:04:56.894526: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "/kernel/lib/python3.8/site-packages/ml_kernel/ignored_keyboard_interrupt.py:16: UserWarning: State committing stage cannot be interrupted. Please wait.\n",
      "  warnings.warn(self._warn_message)\n",
      "/kernel/lib/python3.8/site-packages/ml_kernel/kernel.py:873: UserWarning: The following variables cannot be serialized: data, history, history_list, modelUnet\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Обучаем на нескольких выборках\n",
    "N_np = 5\n",
    "start_from = 1 # нумерации файлов \n",
    "\n",
    "history_list = []\n",
    "\n",
    "# Загружаем модель (поправить вручную)\n",
    "model_file = os.path.join(model_path, '00-Unet')\n",
    "modelUnet = load_model(model_file, custom_objects={'dice_coef':dice_coef})\n",
    "\n",
    "cur_time = time.time()\n",
    "for n in range(N_np):\n",
    "    np_file = np_files[start_from + n - 1]\n",
    "    data = np.load(os.path.join(data_path, np_file))\n",
    "    images_np = data['a']\n",
    "    anns_np = data['b']\n",
    "    print(\"\\nОбучение на выборке {}\".format(np_file))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(images_np, anns_np, test_size=0.15, shuffle=True)   \n",
    "    \n",
    "    history = modelUnet.fit(x_train, y_train, epochs=10, batch_size=2, validation_data = (x_val, y_val)) \n",
    "    history_list.append(history)\n",
    "    \n",
    "    file_number = \"%02d\" % (start_from + n)\n",
    "    model_file = os.path.join(model_path, file_number + '-Unet')\n",
    "    modelUnet.save(model_file, save_format='tf')\n",
    "    \n",
    "print(\"Время выполнения: \", round(time.time() - cur_time, 2), 'c', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "cellId": "yrxqhwjbkdoz0j1ukr3sr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-781c3296f5ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhistory_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhistory_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mN\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mhistory_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_acc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhistory_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_coef'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mhistory_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_val\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhistory_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_dice_coef'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_list' is not defined"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "#Отображаем график точности в процессе обучения\n",
    "history_acc = []\n",
    "history_val = []\n",
    "for N in range(len(history_list)):\n",
    "    history_acc = history_acc + history_list[N].history['dice_coef']\n",
    "    history_val = history_val + history_list[N].history['val_dice_coef']\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(history_acc, \n",
    "         label='dice_coef')\n",
    "plt.plot(history_val, \n",
    "         label='val_dice_coef')\n",
    "plt.xlabel('Эпоха обучения')\n",
    "plt.ylabel('Метрики')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Максимальная точность\n",
    "val_acc = np.array(history_val)\n",
    "print('Максимальная точность val_dice_coef = {} на эпохе {}.'.format(round(val_acc.max(),4), val_acc.argmax()))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Необработанный формат ячейки",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "f92a7050-ed90-4ba1-bfee-acbd3667b5a3",
  "notebookPath": "NIIAS-RZD/Модель Unet.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
